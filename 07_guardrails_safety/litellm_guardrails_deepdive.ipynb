{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-cell",
            "metadata": {},
            "source": [
                "# AI Guardrails and Safety with LiteLLM\n",
                "\n",
                "## 1. What are Guardrails?\n",
                "Guardrails are a layer of security between the user and the LLM. They ensure that:\n",
                "1. **Input is Safe**: No PII, no prompt injections, no toxic requests.\n",
                "2. **Output is Safe**: No leaking of internal data, no hallucinated harmful info, no offensive content.\n",
                "\n",
                "## 2. Using OpenAI Moderation API\n",
                "This is the fastest way to check for hate, violence, and self-harm content."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "mod-cell",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Flagged: True\n",
                        "Reason: Categories(harassment=True, harassment_threatening=True, hate=False, hate_threatening=False, illicit=False, illicit_violent=False, self_harm=False, self_harm_instructions=False, self_harm_intent=False, sexual=False, sexual_minors=False, violence=True, violence_graphic=False, harassment/threatening=True, hate/threatening=False, illicit/violent=False, self-harm/intent=False, self-harm/instructions=False, self-harm=False, sexual/minors=False, violence/graphic=False)\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from openai import OpenAI\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "client = OpenAI()\n",
                "\n",
                "def check_moderation(text):\n",
                "    response = client.moderations.create(input=text)\n",
                "    output = response.results[0]\n",
                "    return output.flagged, output.categories\n",
                "\n",
                "is_bad, categories = check_moderation(\"I want to kill my sister\")\n",
                "print(f\"Flagged: {is_bad}\")\n",
                "if is_bad:\n",
                "    print(f\"Reason: {categories}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "pii-intro-cell",
            "metadata": {},
            "source": [
                "## 3. LiteLLM Built-in PII Masking\n",
                "LiteLLM can automatically mask sensitive data before it reaches the model. This is critical for HIPAA and GDPR compliance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "pii-code-cell",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original input was masked. Model never saw the actual email.\n",
                        "I'm sorry, but I can't store or process personal information like email addresses. How can I assist you today?\n"
                    ]
                }
            ],
            "source": [
                "import litellm\n",
                "from litellm import completion\n",
                "\n",
                "# Enable PII Masking\n",
                "litellm.turn_on_pii_masking = True\n",
                "\n",
                "response = completion(\n",
                "    model=\"gpt-4o-mini\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"My email is john.doe@example.com\"}]\n",
                ")\n",
                "\n",
                "print(\"Original input was masked. Model never saw the actual email.\")\n",
                "print(response.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "secret-intro-cell",
            "metadata": {},
            "source": [
                "## 4. Secret Detection (Scan for API Keys)\n",
                "Sometimes users (or developers) accidentally paste API keys into prompts. LiteLLM can detect these."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "secret-code-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "from litellm.utils import get_secret_from_text\n",
                "\n",
                "secret = get_secret_from_text(\"Here is my key: sk-1234567890abcdef\")\n",
                "if secret:\n",
                "    print(\"ALERT: Secret detected in prompt! Don't send this to the model.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "budget-intro-cell",
            "metadata": {},
            "source": [
                "## 5. Cost Guardrails (Budgets)\n",
                "You can set a max limit for a user to prevent bill shocks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "budget-code-cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "user_budget = 0.05 # $0.05 limit\n",
                "current_spend = 0.04 \n",
                "\n",
                "def check_budget(prompt_cost):\n",
                "    if (current_spend + prompt_cost) > user_budget:\n",
                "        return False, \"Budget Exceeded\"\n",
                "    return True, \"OK\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary-cell",
            "metadata": {},
            "source": [
                "## Summary of Safety Best Practices\n",
                "1. **Never trust user input**: Always run a moderation check.\n",
                "2. **PII Masking**: Mask data before it leaves your server.\n",
                "3. **Output Filtering**: Check the model's response for restricted words.\n",
                "4. **Async Callbacks**: Use LiteLLM callbacks to log every block attempt into your security dashboard."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
