{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b9cd3f",
   "metadata": {},
   "source": [
    "### Static model\n",
    "#### Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach. To initialize a static model from a model identifier string \n",
    "\n",
    "#### Eg: \"openai:gpt-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fa507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    " \n",
    "agent = create_agent(\"openai:gpt-5\", tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91908a",
   "metadata": {},
   "source": [
    "#### For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use ChatOpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    timeout=30\n",
    "    # ... (other params)\n",
    ")\n",
    "#agent = create_agent(model, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a301c1",
   "metadata": {},
   "source": [
    "### Dynamic model\n",
    "#### Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware using the @wrap_model_call decorator that modifies the model in the request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    return handler(request.override(model=model))\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    #tools=tools,\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad658f",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d384db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    return f\"Weather in {location}: Sunny, 72Â°F\"\n",
    "\n",
    "agent = create_agent(model, tools=[search, get_weather])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccd5dd0",
   "metadata": {},
   "source": [
    "### Tool error handling\n",
    "#### To customize how tool errors are handled, use the @wrap_tool_call decorator to create middleware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"What's the weather like in New York?\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
