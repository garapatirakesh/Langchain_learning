{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48552778",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "LangChain Expression Language (LCEL) is the recommended approach to building chains in LangChain, having superseded the traditional methods (including LLMChain). LCEL gives us a more flexible system for building chains. The pipe operator | is used by LCEL to chain together components. Let's see how we'd construct an LLMChain using LCEL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traditional chains \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a329bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Give me a small report on {topic}\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    " # input | prompt_template | llm | output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c3b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt , output_parser=output_parser)\n",
    "\n",
    "result = chain.invoke(\"langchain expression language\")\n",
    "print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Through the LLMChain class, we can place each of our components in a linear chain.\n",
    "\n",
    "from langchain_classic.chains import LLMChain\n",
    "#from langchain.chains import LLMChain\n",
    "\n",
    "lcel_chain = prompt | llm | output_parser\n",
    "\n",
    "result = lcel_chain.invoke(\"langchain expression language\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba145bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1494fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other.invoke(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37969c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5   # 3+5 =8\n",
    "\n",
    "def sub_two(x):\n",
    "    return x-2   # 3-2  or 8 -2\n",
    "\n",
    "def mul_seven(x):\n",
    "    return x*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b375995",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_five_runnable = Runnable(add_five)\n",
    "sub_five_runnable = Runnable(sub_two)\n",
    "mul_five_runnable = Runnable(mul_seven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf2f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
    "\n",
    "chain.invoke(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4be7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(3).__add__(5)  # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f1e50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f545b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
    "\n",
    "chain.invoke(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa412a",
   "metadata": {},
   "source": [
    "# LCEL RunnableLambda\n",
    "The RunnableLambda class is LangChain's built-in method for constructing a runnable object from a function. It does the same thing as the custom Runnable class we created earlier. Let's try it out with the same functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7aa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_five(x):\n",
    "    return x+5   # 3+5 =8\n",
    "\n",
    "def sub_two(x):\n",
    "    return x-2   # 3-2  or 8 -2\n",
    "\n",
    "def mul_seven(x):\n",
    "    return x*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171730d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_five_runnable = RunnableLambda(add_five)\n",
    "sub_five_runnable = RunnableLambda(sub_two)\n",
    "mul_five_runnable = RunnableLambda(mul_seven)\n",
    "\n",
    "chain = add_five_runnable | sub_five_runnable | mul_five_runnable \n",
    "chain.invoke(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try something complex\n",
    "prompt_str = \"give me a small report about {topic}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=prompt_str\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke(\"AI\")\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9477f",
   "metadata": {},
   "source": [
    "### Here, we define two functions: extract_fact will remove the first paragraph (typically the introduction), and replace_word will replace the substring \"RAG\" with \"hot potato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "old_word = \"RAG\"\n",
    "new_word = \"hot potato\"\n",
    "\n",
    "def replace_word(x):\n",
    "    return x.replace(old_word, new_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d3fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_fact_runnable = RunnableLambda(extract_fact)\n",
    "replace_word_runnable = RunnableLambda(replace_word)\n",
    "\n",
    "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable\n",
    "result = chain.invoke(\"retrieval augmented generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c684a",
   "metadata": {},
   "source": [
    "# LCEL RunnableParallel and RunnablePassthrough\n",
    "LCEL provides us with various Runnable classes that allow us to control the flow of data and execution order through our chains. Two of these are RunnableParallel and RunnablePassthrough.\n",
    "\n",
    "RunnableParallel allows us to run multiple Runnable instances in parallel, acting almost like a Y-fork in the chain.\n",
    "\n",
    "RunnablePassthrough â€” allows us to pass through a variable to the next Runnable without modification.\n",
    "\n",
    "To see these runnable in action, we will create two data sources. Each source provides specific information, but to answer the question, we will need both to be fed to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-community\n",
    "#!pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c7a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "\n",
    "\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "        [\"half the info is here\", \"James' birthday is December the 7th\"],\n",
    "        embedding=embedding,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "        [\"the other half of the info is here\", \"James was born in 1994\"],\n",
    "        embedding=embedding,\n",
    "    )\n",
    "\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"Missing package required for DocArrayInMemorySearch:\", e)\n",
    "    print(\"Install with: python -m pip install docarray (activate your venv first).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96ec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
    "Context: \n",
    "{context_a}\n",
    "{context_b}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    " ChatPromptTemplate,\n",
    " SystemMessagePromptTemplate,\n",
    " HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    " SystemMessagePromptTemplate.from_template(prompt_str),\n",
    " HumanMessagePromptTemplate.from_template(\"{question}\") \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca23b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | llm | output_parser\n",
    "result = chain.invoke(\"What was the date when James was born\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
