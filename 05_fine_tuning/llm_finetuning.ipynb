{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning: A Comprehensive Guide\n",
    "\n",
    "## Introduction\n",
    "Fine-tuning is the process of training a pre-trained Large Language Model (LLM) on a specific dataset to adapt it for a particular task, style, or domain.\n",
    "\n",
    "### Fine-Tuning vs. RAG\n",
    "- **RAG (Retrieval-Augmented Generation)**: Provides *context* to the model at inference time. The model's weights do not change. Good for up-to-date knowledge.\n",
    "- **Fine-Tuning**: Modifies the model's *internal weights*. Good for changing the *behavior*, *style*, or teaching a strict format/syntax.\n",
    "\n",
    "## Fine-Tuning Methods\n",
    "We will cover the main approaches, focusing on modern efficient methods.\n",
    "\n",
    "1. **Full Fine-Tuning**: Updates all billions of parameters. Extremely computationally expensive (requires massive GPU clusters).\n",
    "2. **PEFT (Parameter-Efficient Fine-Tuning)**: Updates only a small number of extra parameters while freezing the main model. \n",
    "   - **LoRA (Low-Rank Adaptation)**: The most popular method. Injects small rank-decomposition matrices into layers.\n",
    "   - **QLoRA (Quantized LoRA)**: Uses 4-bit quantization for the base model to drastically reduce memory usage (run 7B models on free Colab/consumer GPUs).\n",
    "   - **Prefix Tuning / Prompt Tuning**: Optimizes a continuous vector (virtual tokens) added to the input.\n",
    "3. **Alignment Tuning**:\n",
    "   - **RLHF (Reinforcement Learning from Human Feedback)**: Uses a reward model to train the LLM.\n",
    "   - **DPO (Direct Preference Optimization)**: A more stable alternative to RLHF that optimizes essentially the same objective without a separate reward model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "We need `transformers`, `peft`, `trl` (Transformer Reinforcement Learning - library by HuggingFace for SFT/DPO), `bitsandbytes`, and `accelerate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers peft trl bitsandbytes accelerate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "Standard instruction datasets usually have columns like `instruction`, `input`, `output` (Alpaca format) or `messages` (ChatML format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example: Using a tiny subset of an instruction dataset or creating a dummy one\n",
    "data = [\n",
    "    {\"text\": \"Human: What is the capital of France?\\nAssistant: The capital of France is Paris.\"},\n",
    "    {\"text\": \"Human: Explain Quantum Computing.\\nAssistant: It uses quantum bits (qubits) to perform complex calculations.\"}\n",
    "]\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. QLoRA: Efficient Fine-Tuning\n",
    "We will load a model in 4-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# 4-bit Quantization Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LoRA Configuration\n",
    "Define which modules we are targeting (usually `q_proj`, `v_proj`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,        # Rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with SFTTrainer\n",
    "The `SFTTrainer` (Supervised Fine-tuning Trainer) from `trl` simplifies the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    max_steps=50, # Short run for demo\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# trainer.train() # Uncomment to run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving and Merging\n",
    "After training, you have \"adapter weights\". To use the model normally, you merge these weights back into the base model.\n",
    "\n",
    "```python\n",
    "trainer.model.save_pretrained(\"final_adapter\")\n",
    "# Reload base and adapter, then merge\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, ...)\n",
    "model = PeftModel.from_pretrained(base_model, \"final_adapter\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"merged_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Other Methods Briefly Explained\n",
    "\n",
    "### Instruction Tuning\n",
    "This is fine-tuning on a dataset formatted as Q&A or Instructions (like the one above). It turns a raw text-completion model into a helpful assistant.\n",
    "\n",
    "### DPO (Direct Preference Optimization)\n",
    "Instead of just mimicking text (SFT), DPO takes a dataset of `(prompt, chosen_response, rejected_response)`. It increases the probability of the chosen response and decreases the rejected one. This is key for making models \"safer\" or aligned with human preferences.\n",
    "\n",
    "```python\n",
    "from trl import DPOTrainer\n",
    "# Requires a dataset with columns: prompt, chosen, rejected\n",
    "# trainer = DPOTrainer(model, ref_model, args=..., train_dataset=dpo_dataset)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
