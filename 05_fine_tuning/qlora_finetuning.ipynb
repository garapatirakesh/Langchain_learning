{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning with QLoRA (Quantized LoRA)\n",
                "\n",
                "## 1. What is Quantization?\n",
                "Quantization is the process of reducing the precision of the model's weights. Large Language Models (LLMs) are typically stored in **FP32** (32-bit floating point) or **BF16/FP16** (16-bit).\n",
                "\n",
                "By quantizing to **4-bit**, we reduce the memory footprint by almost 8x, allowing us to run and fine-tune large models (like Llama-3 8B or Mistral 7B) on consumer-grade GPUs (like an RTX 3090 or even free Google Colab T4).\n",
                "\n",
                "## 2. What is QLoRA?\n",
                "**QLoRA** (Quantized Low-Rank Adaptation) is an efficient fine-tuning technique that:\n",
                "1.  **Quantizes** the base model to 4-bit (using the NormalFloat4 or NF4 data type).\n",
                "2.  **Freezes** the base model weights.\n",
                "3.  **Adds** small, trainable 16-bit adapter weights (LoRA layers) that are optimized during training.\n",
                "\n",
                "This combination gives us the performance of full fine-tuning with a fraction of the memory."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Setup\n",
                "We need `bitsandbytes` (for quantization), `peft` (for LoRA), and `trl` (for easy training)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install -q -U bitsandbytes transformers peft accelerate datasets trl"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Loading the Model in 4-bit\n",
                "We use `BitsAndBytesConfig` to define the 4-bit quantization parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # Low memory model for demo\n",
                "\n",
                "quant_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,       # Quantize the quantization constants\n",
                "    bnb_4bit_quant_type=\"nf4\",            # NormalFloat4 is better than pure 4-bit\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16 # Computation still happens in 16-bit\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=quant_config,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Preparing for Training\n",
                "We need to specifically prepare the quantized model for k-bit training using `prepare_model_for_kbit_training`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import prepare_model_for_kbit_training\n",
                "\n",
                "model.gradient_checkpointing_enable()\n",
                "model = prepare_model_for_kbit_training(model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Configuring LoRA Adapters\n",
                "This is where we define the 'bottleneck' layers that will actually be trained."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import LoraConfig, get_peft_model\n",
                "\n",
                "config = LoraConfig(\n",
                "    r=16, # Rank: the size of the temporary matrices\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], # Modules to adapt\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\"\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, config)\n",
                "\n",
                "# Check how many parameters we are actually training\n",
                "print(f\"Trainable parameters: {model.get_nb_trainable_parameters()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Dataset and Training\n",
                "We use the `SFTTrainer` (Supervised Fine-Tuning Trainer) which handles the complexity of LoRA under the hood."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Dummy dataset for example\n",
                "dataset = load_dataset(\"json\", data_files={\"train\": [{\"text\": \"### Instruction: Hello\\n### Response: Hi there!\"}]})\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./outputs\",\n",
                "    per_device_train_batch_size=1,\n",
                "    gradient_accumulation_steps=4,\n",
                "    learning_rate=2e-4,\n",
                "    max_steps=10,\n",
                "    logging_steps=1,\n",
                "    fp16=True if torch.cuda.is_available() else False,\n",
                "    optim=\"paged_adamw_8bit\" # Special optimizer for quantization\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset[\"train\"],\n",
                "    dataset_text_field=\"text\",\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "# trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary of QLoRA Workflow\n",
                "1.  **Quantize model** to 4-bit (NF4) to save memory.\n",
                "2.  **Freeze** original weights.\n",
                "3.  **Apply LoRA** to specific layers.\n",
                "4.  **Train** ONLY the LoRA adapters.\n",
                "5.  **Merge** adapters back into the base model (optional, for deployment)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
